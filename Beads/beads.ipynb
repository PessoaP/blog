{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling probability arrays: a  Python class\n",
    "\n",
    "When working in data science, we often need arrays of weights that sum to one, we call these probability arrays or probability vectors. \n",
    "For example, in classification tasks, we want the output to be a array of probabilities representing predicted class distributions. When implement mixture models, probability arrays can represent the weight of each component. In finance, they might define the weights of assets in a portfolio where the total allocation must sum to one.\n",
    "\n",
    "However, ensuring that these vectors are correctly sampled and always sum to one is not trivial. A simple approach, like sampling random numbers and normalizing them, can lead to uneven or biased distributions. So, how can we guarantee that sampled arrays always meet this constraint?\n",
    "\n",
    "One way to do so is to imagine placing beads into bins. Picture having a finite number of beads, and distributing them across five bins. Each bin represents one component of the probability array, and the fraction of beads in each bin determines the corresponding probability weight. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"https://github.com/PessoaP/blog/blob/master/Beads/beads1.png?raw=true\" alt=\"S\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analogy is not only intuitive but also robust, as it naturally ensures that the total sum is preserved without needing normalization after. Additionally, this representation provides flexibility: by increasing the number of beads, we can achieve higher precision in our probability arrays. Moreover, this bead-and-bin model offers a precise way to represent probabilities. Since beads are discrete, the total count is inherently stable, avoiding the floating-point errors that can arise when dealing with continuous random variables.\n",
    "\n",
    "In this blog post, we build a Python class called `prob_array` to model probability arrays using the bead-and-bin analogy. We'll cover three core functions: (i) initializing arrays from raw counts or by another probability vector, (ii) proposing symmetric updates by redistributing beads; and (iii) calculating the log-probability under a multinomial prior, including necessary mathematical corrections. \n",
    "By the end, you'll have a clean implementation of `prob_array`, with examples showcasing initialization, symmetric proposals, and probabilistic evaluation. Let’s begin with array initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array initialization\n",
    "First, let us creat a method for array initialization that can handle 3 types of input:\n",
    "\n",
    "- No input array: The class initializes a balanced distribution where each component has an equal number of beads.\n",
    "\n",
    "- Integer input: The input is assumed to represent bead counts directly.\n",
    "\n",
    "- Probability array input: The array is scaled to match the total bead count while preserving proportions as much as possible.\n",
    "\n",
    "This is implemented as the Python class below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "normalize = lambda x: x/x.sum()\n",
    "\n",
    "class prob_array:\n",
    "    def __init__(self,array=None,components=20,beads=10000):\n",
    "        if array is None:\n",
    "            self.counts = np.ones(components,dtype=int)*(beads//components)\n",
    "            self.counts[:beads%components]+=1\n",
    "        elif array.dtype == int:\n",
    "            self.counts = array\n",
    "        elif np.isclose(1.,np.sum(array)):\n",
    "            self.counts = np.floor(array*beads).astype(int)\n",
    "            self.counts[np.argsort(array)[:beads%self.counts.sum()]]+=1\n",
    "\n",
    "        self.prob = normalize(self.counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposing symmetric updates\n",
    "\n",
    "Once we have a probability array, we may want to generate new samples that are close to the array we have while ensuring they still sum to 1. This is particularly relevant in Markov Chain Monte Carlo (MCMC) methods, such as Metropolis-Hastings, where it is convenient to use symmetric proposals as they help simplify computations. In this context, a symmetric proposal means that the probability of transitioning from state $A$ to state $B$ is equal to the probability of transitioning from $B$ to $A$. \n",
    "\n",
    "The `proposal` function below implements it as a bead-movement scheme. Each bead has a probability $r$ to move, being $r/2$ of moving to the left bin and $r/2$ of moving to the right bin, staying in the same place with probability $1-r$. If it is in the first bin and it moves left (or in the las bin and move right) it just stays in place\n",
    "<figure>\n",
    "  <img src=\"https://github.com/PessoaP/blog/blob/master/Beads/beads2.png?raw=true\" alt=\"S\"/>\n",
    "</figure>\n",
    "Note that for each bead, the movements are symmetric, meaning each bead has an equal probability of moving to a neighboring bin as a bead in that bin has of moving back, the proposal remains symmetric.\n",
    "\n",
    "\n",
    "We implement a function doing this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal(p_array, rate=0.01):\n",
    "    movables = np.random.binomial(p_array.counts, rate)  # Select beads to move\n",
    "    new_counts = p_array.counts - movables \n",
    "\n",
    "    mvleft = np.random.binomial(movables, 0.5)  # Half move left\n",
    "    mvright = movables - mvleft  # Half move right\n",
    "\n",
    "    new_counts[:-1] += mvleft[1:]\n",
    "    new_counts[0] += mvleft[0]  # Beads from the first bin stay in place\n",
    "\n",
    "    new_counts[1:] += mvright[:-1]\n",
    "    new_counts[-1] += mvright[-1]  # Beads from the last bin stay in place\n",
    "\n",
    "    return prob_array(new_counts,new_counts.size,new_counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function maintains the total bead count while making small perturbations. The `rate` parameter  (equivalent to $r$) controls the fraction of beads that move at each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Probability with a Multinomial Model\n",
    "\n",
    "Now, suppose we want to evaluate the likelihood that a bead-and-bin configuration was generated from a multinomial distribution. This approach provides a specific way to define what [Bayesian statistics refer to as a prior](https://labpresse.com/why-do-we-need-bayesian-statistics-part-i-asserting-if-a-coin-is-biased-tutorial/), though that is beyond our focus here.\n",
    "\n",
    "A multinomial model assumes that each bead is assigned a priori to each bin with a probability given by another probability vector, $\\alpha = \\{\\alpha_1, alpha_2, \\ldots, \\alpha_n\\}$, which represents the expected proportions of beads across bins. \n",
    "\n",
    "Given a total number of beads, the multinomial model determines the probability is given by \n",
    "$$ p(\\{k_i\\}|\\{\\alpha_i\\},N) = \\frac{N!}{k_1! k_2! \\ldots k_I!}  \\alpha_1^{k_1} \\alpha_2^{k_2} \\ldots \\alpha_I^{k_I} $$\n",
    "\n",
    "Where \n",
    "- $N$ is the total number of beads\n",
    "- $k_i$ represents the number of beads in the $i$-th bin, with $I$ being the total number of beads (`components`)\n",
    "- $\\alpha_i$ the probability a bead was assigned to the $i$-th bead\n",
    "With the factorial prefactor acounting for the number of ways to arrange the beads among the bins such that one still have the same $k_i$.\n",
    "\n",
    "When dealing with probabilities working with their logarithm simplifies computations and mitigates numerical underflow. Using the property that the logarithm of a product is the sum of the logarithms, we take the log of the multinomial probability\n",
    "$$ \\log p(\\{k_i\\}|\\{\\alpha_i\\},N) = \n",
    "\\log N! - \\sum_{i=1}^I  \\log( k_i! ) + \\sum_{i=1}^I {k_i} \\log(\\alpha_i)  . $$\n",
    "\n",
    "An interesting trick at this stage is to use the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function). Although much can be said about it, here we will simply note that the Gamma function is a well-known mathematical function that, for positive integers, matches the factorial as $\\Gamma(n) = (n-1)!$.  Thus, instead of computing the logarithm of a factorial, we can use the logarithm of the Gamma function, which is implemented in libraries such as [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.loggamma.html). Leading us to the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import loggamma\n",
    "logfactorial = lambda x: loggamma(x+1)    \n",
    "def multinomial_logprob(p_array,alpha):\n",
    "    N = p_array.counts.sum()\n",
    "    alpha = normalize(alpha)\n",
    "    return logfactorial(N) - logfactorial(p_array.counts).sum() + (p_array.counts*np.log(alpha)).sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it a single class\n",
    "\n",
    "For ease of use in Python we can combine all of the functionnality meantioned here into a single class that is structured and reusable. This serves as an exercise in object-oriented programming while also making it easier to handle probability arrays, generate proposals, and compute log-probabilities in a unified interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import loggamma\n",
    "\n",
    "# Utility functions\n",
    "normalize = lambda x: x / x.sum()  # Ensures the input array sums to 1\n",
    "logfactorial = lambda x: loggamma(x + 1)  # Computes the log-factorial using log-gamma\n",
    "\n",
    "class prob_array:\n",
    "    def __init__(self, array=None, components=20, beads=10000):\n",
    "        \"\"\"\n",
    "        Initialize a probability array.\n",
    "        \n",
    "        Parameters:\n",
    "        - array: Optional numpy array, can represent counts or probabilities.\n",
    "        - components: Number of components in the probability array.\n",
    "        - beads: Total number of beads (samples) for normalization.\n",
    "        \"\"\"\n",
    "        if array is None:\n",
    "            # Uniform distribution of beads across components\n",
    "            self.counts = np.ones(components, dtype=int) * (beads // components)\n",
    "            self.counts[:beads % components] += 1  # Distribute remaining beads\n",
    "        elif array.dtype == int:\n",
    "            # If array represents counts directly\n",
    "            self.counts = array\n",
    "        elif np.isclose(1.0, np.sum(array)):\n",
    "            # If array represents probabilities\n",
    "            self.counts = np.floor(array * beads).astype(int)\n",
    "            remainder = beads - self.counts.sum()\n",
    "            self.counts[np.argsort(array)[-remainder:]] += 1  # Adjust for rounding errors\n",
    "        else:\n",
    "            raise ValueError(\"Input array must be counts (int) or probabilities (sum to 1).\")\n",
    "        \n",
    "        self.prob = normalize(self.counts)  # Normalize counts to probabilities\n",
    "\n",
    "    def proposal(self, rate=0.01):\n",
    "        \"\"\"\n",
    "        Generate a proposal for a new probability array.\n",
    "        \n",
    "        Parameters:\n",
    "        - rate: Rate of change for beads redistribution.\n",
    "        \n",
    "        Returns:\n",
    "        - New prob_array instance with adjusted counts.\n",
    "        \"\"\"\n",
    "        movables = np.random.binomial(self.counts, rate)  # Determine movable beads\n",
    "        new_counts = self.counts - movables\n",
    "\n",
    "        # Redistribute beads left and right\n",
    "        mvleft = np.random.binomial(movables, 0.5)\n",
    "        mvright = movables - mvleft\n",
    "\n",
    "        new_counts[:-1] += mvleft[1:]\n",
    "        new_counts[0] += mvleft[0]  # Beads attempting to move left from the first index stay\n",
    "\n",
    "        new_counts[1:] += mvright[:-1]\n",
    "        new_counts[-1] += mvright[-1]  # Beads attempting to move right from the last index stay\n",
    "\n",
    "        # Ensure counts remain valid\n",
    "        if np.any(new_counts < 0):\n",
    "            raise ValueError(\"Invalid move: Negative counts detected.\")\n",
    "        \n",
    "        return prob_array(new_counts, new_counts.size, new_counts.sum())\n",
    "    \n",
    "    def multinomial_logprob(self, alpha):\n",
    "        \"\"\"\n",
    "        Compute the log-probability of the current counts given a multinomial distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        - alpha: probability assigned to each bin\n",
    "        \n",
    "        Returns:\n",
    "        - Log-probability value.\n",
    "        \"\"\"\n",
    "    \n",
    "        N = self.counts.sum()\n",
    "        alpha = normalize(alpha)\n",
    "        return logfactorial(N) - logfactorial(self.counts).sum() + (self.counts*np.log(alpha)).sum() \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"prob_array(counts={self.counts}, prob={self.prob})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this trully necessary?\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://i.kym-cdn.com/entries/icons/original/000/028/596/dsmGaKWMeHXe9QuJtq_ys30PNfTGnMsRuHuo_MUzGCg.jpg\" alt=\"S\" width=\"320\"/>\n",
    "</figure>\n",
    "\n",
    "Not really.\n",
    "\n",
    "\n",
    "While sampling probability vectors is important in data science, alternatives exist, such as using the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution), whose sampler is [already implemented in `numpy`](https://numpy.org/doc/2.1/reference/random/generated/numpy.random.dirichlet.html). Dirichlet samples are guaranteed to be valid probability vectors while using only continuous mathematics rather than the bead discretization here.\n",
    "\n",
    "Moreover, while symmetric proposals are often used in Metropolis-Hastings, they are not strictly required as long as you can calculate the probability of sampling from the proposal distribution.\n",
    "\n",
    "Therefore, the Python class developed here, while pedagogically interesting, does not solve a fundamental problem in data science. I have to admit, I originally wrote this years ago as an unnecessary but intriguing attempt to reinvent the wheel. Still, I’m sharing it here in case someone finds it useful or simply enjoyable to explore.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
